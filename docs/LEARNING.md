
### 2.3 : [Concept 1] # Advanced Data Fetching: Static, Dynamic, and Hybrid Rendering in the App Router

## üîπ Rendering Strategies Used

### 1. Static Rendering (SSG)

* Pages are generated at **build time**
* Very fast and scalable
* Used for pages with content that rarely changes
  **Example:** About / Blog page

```js
export const revalidate = false;
```

---

### 2. Dynamic Rendering (SSR)

* Pages are generated on **every request**
* Always shows fresh, real-time data
* Used for user-specific or live data
  **Example:** Dashboard / Profile page

```js
export const dynamic = 'force-dynamic';
```

```js
const data = await fetch(url, { cache: 'no-store' });
```

---

### 3. Hybrid Rendering (ISR)

* Pages are static but **revalidate after a fixed time**
* Balances performance and fresh data
* Used for frequently updated content
  **Example:** News / Events / Products page

```js
export const revalidate = 60;
```

---

## üöÄ Why These Approaches Were Chosen

* **Static pages** ‚Üí Best performance and low cost
* **Dynamic pages** ‚Üí Required for real-time and personalized data
* **Hybrid pages** ‚Üí Good balance between speed and data freshness

---

## üß† Key Learnings

* Not all pages need real-time rendering
* Choosing the right strategy improves performance and scalability
* Next.js App Router makes rendering control simple and flexible

---

## ‚úÖ Conclusion

Using Static, Dynamic, and Hybrid rendering together helps build fast, scalable, and real-world production-ready applications. The App Router allows selecting the best strategy per page based on data requirements.








### 2.5 : [Concept 3] # Cloud Deployments 101: Docker ‚Üí CI/CD ‚Üí AWS/Azure

## Question
How do Docker and CI/CD pipelines simplify deployment workflows, and what considerations are important when deploying a full-stack application securely to AWS or Azure?

---

## 1. How Docker Simplifies Deployment Workflows

Docker packages an application and all its dependencies into a single, portable container image. This guarantees that the application runs the same in development, CI, and production.

In our project, the Next.js application is containerized using a Dockerfile. PostgreSQL and Redis run as separate containers during development using Docker Compose. The same Docker image is deployed to AWS/Azure.

**Benefits:**
- Eliminates ‚Äúworks on my machine‚Äù issues  
- Ensures environment consistency  
- Makes deployments portable and repeatable  

Docker stabilizes **what** we deploy.

---

## 2. How CI/CD Pipelines Simplify Deployment

CI/CD pipelines automate build, test, and deploy steps. Every code commit follows the same path.

Using GitHub Actions, our pipeline:
1. Triggers on push  
2. Runs checks  
3. Builds a Docker image  
4. Applies migrations  
5. Deploys to AWS/Azure  

**Benefits:**
- Fewer human errors  
- Predictable releases  
- Faster feedback  

CI/CD stabilizes **how** we deploy.

---

## 3. Secure Deployment Considerations (AWS / Azure)

### Secrets Management
- No secrets in code or images  
- Use AWS Secrets Manager / Azure Key Vault  
- Inject at runtime  

### Environment Separation
- Dev ‚â† Prod  
- Separate configs and credentials  

### Network Security
- DB/Redis private  
- App public  

### Access Control
- IAM roles / Managed identities  
- No hardcoded credentials  

---

## 4. Case Study: The Never-Ending Deployment Loop

**Chain of Trust**
Code Commit ‚Üí Docker Image ‚Üí CI/CD ‚Üí Cloud Deployment ‚Üí Runtime Environment

### Issues
- **Env var not found:** secrets not injected/validated  
- **Port already in use:** old containers not stopped  
- **Old container running:** no versioning/cleanup  

---

## 5. Fix with Proper Docker & CI/CD

### Containerization
- Versioned Docker images  
- No secrets baked in  

### Pipeline
1. Validate env vars  
2. Stop/remove old container  
3. Deploy new version  
4. Health check  
5. Rollback on failure  

Restores the chain of trust.

---

## Conclusion
Docker ensures consistency of artifacts; CI/CD ensures consistency of delivery. Secure configuration and clean handoffs prevent deployment loops.

<br>

### 2.9 :  # TypeScript & ESLint Configuration


## Question

## 1. Why Strict TypeScript Mode Reduces Runtime Bugs

Strict mode acts as a safety net that catches errors **before** you run the code.

*   **Catches "Null" Errors:** Prevents crashes by forcing you to handle missing data (`null`/`undefined`).
*   **No Guesswork:** Stops you from using variables without explicit types ("implicit any").
*   **Early Detection:** Finds typos and logic flaws during coding, not in production.

**Impact:** A "compile-time" spell-checker that prevents simple mistakes from becoming production crashes.

---

## 2. What ESLint + Prettier Rules Enforce

*   **ESLint (Quality Control):** Catch logical errors, unused variables, and bad practices (e.g., "don't use `var`").
*   **Prettier (Style Enforcer):** Automatically formats code (spacing, commas, semicolons) on save.

**Impact:** No more debates about code style. The codebase looks like it was written by one person, regardless of team size.

---

## 3. How Pre-Commit Hooks Improve Team Consistency

Tools like **Husky** run checks automatically when you try to commit code.

*   **Gatekeeper:** Blocks commits if linting fails or tests don't pass.
*   **Automation:** Ensures no one "forgets" to format their code or check for errors.
*   **Shared Standards:** Enforces the same rules for every developer on the team.

**Impact:** Keeps the "main" branch clean. Broken or messy code never leaves your local machine.

---

## Conclusion
TypeScript prevents bugs. ESLint/Prettier clean up the mess. Husky locks the door so bad code can't get out. Together, they guarantee a professional, stable codebase.



## 2.10 Environment Variable Management

This project uses environment variables to securely manage configuration and secrets.

### Environment Files

- `.env.local`
  - Contains real credentials and secrets
  - Used only in local development
  - Never committed to GitHub

- `.env.example`
  - Template file listing all required variables
  - Uses placeholder values
  - Safe to commit and share

### Variable Types

#### Server-side Only
These variables are **never exposed to the browser**:
- `DATABASE_URL`
- `JWT_SECRET`

They are accessed only in server components, API routes, or server actions.

#### Client-side Safe Variables
These variables are safe to expose and **must start with `NEXT_PUBLIC_`**:
- `NEXT_PUBLIC_API_BASE_URL`

### Security Measures

- `.env.local` is ignored using `.gitignore`
- No secrets are hardcoded in the repository
- Client components never access server-only variables

### Common Pitfalls Avoided

- Accidentally exposing secrets by missing `NEXT_PUBLIC_` prefix rules
- Committing real credentials to GitHub
- Using server secrets in client-side components

This setup ensures the application is secure, portable, and production-ready.

## 2.12  Docker & Compose Setup for Local Development

---

### 2.16 : Transactions, Rollbacks, and Query Performance (Prisma)

## Transaction Scenarios (ReliefSync)

**Scenario:** Approving an allocation must update two things together:
1. Update allocation status to **APPROVED**
2. Decrement the source organization inventory

This is implemented in `approveAllocationAndUpdateInventory()` in [src/lib/queries/allocations.ts](src/lib/queries/allocations.ts). The function uses a Prisma transaction so both operations either succeed or roll back together.

---

## Rollback Logic (Atomicity)

Inside the transaction, we validate:
- Allocation exists and is **PENDING**
- Allocation has a **fromOrgId** (source inventory)
- Inventory record exists
- Inventory quantity is sufficient

If any check fails, the function throws an error, and Prisma **rolls back** the entire transaction. This prevents partial writes (e.g., status updated but inventory not decremented).

---

## Query Optimization Applied

### 1. Reduce N+1 Count Queries
The old `countAllocationsByStatus()` loop executed **6 separate queries**. It now uses `groupBy()` to execute **1 query** and maps the result into the same return shape.

**Before:** 6 queries (one per status)
**After:** 1 grouped query

This reduces round trips and improves latency under load.

---

## Indexes Added

To improve allocation lookups by item, an index was added:
- `Allocation.itemId`

Change made in [prisma/schema.prisma](prisma/schema.prisma).

---

## Performance Comparison (Query Count)

**Baseline (before optimization)**
- `countAllocationsByStatus()` ‚Üí 6 count queries

**After optimization**
- `countAllocationsByStatus()` ‚Üí 1 groupBy query

This is a measurable reduction in query count and DB round trips for a common dashboard metric.

---

## Anti-patterns Avoided

- N+1 query loops for counts
- Partial writes without transactions
- Querying unindexed foreign-key fields for high-traffic lists

---

## Reflection: Monitoring in Production

In production, I would monitor:
- **Query latency** (p50/p95)
- **Error rates** for transaction failures
- **Query volume** and slow queries

Tools to use:
- Prisma query logging in staging
- Postgres `EXPLAIN` for hotspots
- APM dashboards (e.g., Grafana + Postgres exporter)


### 3.1 Dockerfile and Docker Compose Setup

We have set up a complete containerized environment for the Next.js application, including a PostgreSQL database and a Redis cache.

#### Dockerfile Usage
The `Dockerfile` is responsible for building the Next.js application image.
- **Base Image:** `node:20-alpine` (Lightweight Node.js environment)
- **Dependencies:** Copies `package.json` and runs `npm install`
- **Build:** Copies source code and runs `npm run build`
- **Execution:** Starts the app using `npm run start`

#### Docker Compose Services
The `docker-compose.yml` orchestrates three services:

1.  **app (Next.js Application)**
    *   Builds from the local `Dockerfile`.
    *   Exposes port `3000`.
    *   Connects to `db` and `redis` services via environment variables.
    *   Depends on `db` and `redis` to be healthy/started.

2.  **db (PostgreSQL)**
    *   Uses `postgres:15-alpine`.
    *   Persists data using a named volume `db_data`.
    *   Exposes port `5432` for local access.

3.  **redis (Redis Cache)**
    *   Uses `redis:7-alpine`.
    *   Exposes port `6379`.

#### Networks and Volumes
- **Network:** `localnet` (Bridge driver) allows all containers to communicate with each other by service name (e.g., `db`, `redis`).
- **Volume:** `db_data` allows the PostgreSQL database to persist data even if the container is removed.

### 3.2 Build and Run Verification

We attempted to run the stack using `docker-compose up --build`.

**Successful Services:**
*   `postgres_db` (Port 5432) - Running
*   `redis_cache` (Port 6379) - Running

**Terminal Output (DB & Redis):**
```
[+] Running 4/4
 ‚úî Network s64-0126-team-02-full-stack-nextjs-reliefsync_localnet  Created
 ‚úî Volume "s64-0126-team-02-full-stack-nextjs-reliefsync_db_data"  Created
 ‚úî Container redis_cache                                           Started
 ‚úî Container postgres_db                                           Started
```

### 3.3 Reflections and Issues Faced

**Issue: Network Connectivity during Build**
While the database and Redis services started successfully, the Next.js application build encountered a network error during `npm install`.

**Error Log:**
```
=> ERROR [4/6] RUN npm install
...
npm error code ECONNRESET
npm error network This is a problem related to network connectivity.
```

**Resolution Strategy (Potential):**
*   This error is likely due to network restrictions or proxy settings within the Docker build environment preventing access to the npm registry.
*   In a production or unrestricted local environment, this command would complete, and the app would start on port 3000.
*   For now, we verified the configuration files are correct and that the supporting infrastructure (DB, Redis) spins up correctly.

---

### 2.17 : RESTful API Design with Next.js App Router

## API Route Hierarchy

The project uses Next.js file-based routing under `app/api/`. Each `route.ts` file exports HTTP method handlers.

```
app/
 ‚îî‚îÄ‚îÄ api/
     ‚îú‚îÄ‚îÄ users/
     ‚îÇ   ‚îú‚îÄ‚îÄ route.ts          # GET (list), POST (create)
     ‚îÇ   ‚îî‚îÄ‚îÄ [id]/
     ‚îÇ       ‚îî‚îÄ‚îÄ route.ts      # GET, PUT, DELETE by ID
     ‚îú‚îÄ‚îÄ organizations/
     ‚îÇ   ‚îú‚îÄ‚îÄ route.ts          # GET (list), POST (create)
     ‚îÇ   ‚îî‚îÄ‚îÄ [id]/
     ‚îÇ       ‚îî‚îÄ‚îÄ route.ts      # GET, PUT, DELETE by ID
     ‚îú‚îÄ‚îÄ allocations/
     ‚îÇ   ‚îú‚îÄ‚îÄ route.ts          # GET (list), POST (create)
     ‚îÇ   ‚îî‚îÄ‚îÄ [id]/
     ‚îÇ       ‚îî‚îÄ‚îÄ route.ts      # GET, PUT, DELETE by ID
     ‚îî‚îÄ‚îÄ inventory/
         ‚îú‚îÄ‚îÄ route.ts          # GET (list), POST (create/update)
         ‚îî‚îÄ‚îÄ [id]/
             ‚îî‚îÄ‚îÄ route.ts      # GET, PUT, DELETE by ID
```

---

## HTTP Verbs and Resource Actions

| HTTP Verb | Route                    | Action                         |
|-----------|--------------------------|--------------------------------|
| GET       | `/api/users`             | List all users (paginated)     |
| POST      | `/api/users`             | Create a new user              |
| GET       | `/api/users/:id`         | Get user by ID                 |
| PUT       | `/api/users/:id`         | Update user by ID              |
| DELETE    | `/api/users/:id`         | Delete user by ID              |
| GET       | `/api/organizations`     | List all organizations         |
| POST      | `/api/organizations`     | Create a new organization      |
| GET       | `/api/organizations/:id` | Get organization by ID         |
| PUT       | `/api/organizations/:id` | Update organization by ID      |
| DELETE    | `/api/organizations/:id` | Delete organization by ID      |
| GET       | `/api/allocations`       | List allocations (filterable)  |
| POST      | `/api/allocations`       | Create allocation request      |
| GET       | `/api/allocations/:id`   | Get allocation by ID           |
| PUT       | `/api/allocations/:id`   | Update allocation status       |
| DELETE    | `/api/allocations/:id`   | Delete allocation              |
| GET       | `/api/inventory`         | List inventory records         |
| POST      | `/api/inventory`         | Create/update inventory        |
| GET       | `/api/inventory/:id`     | Get inventory by ID            |
| PUT       | `/api/inventory/:id`     | Update inventory quantities    |
| DELETE    | `/api/inventory/:id`     | Delete inventory record        |

---

## Sample Requests & Responses

### Get All Users (Paginated)
```bash
curl -X GET "http://localhost:3000/api/users?page=1&limit=10"
```
**Response (200 OK):**
```json
{
  "data": [
    { "id": 1, "email": "alice@ngo.org", "name": "Alice", "role": "NGO" }
  ],
  "pagination": { "page": 1, "limit": 10, "total": 1, "totalPages": 1 }
}
```

### Create a User
```bash
curl -X POST http://localhost:3000/api/users \
  -H "Content-Type: application/json" \
  -d '{"email":"bob@gov.in","name":"Bob","passwordHash":"hashed123","role":"GOVERNMENT"}'
```
**Response (201 Created):**
```json
{
  "message": "User created successfully",
  "data": { "id": 2, "email": "bob@gov.in", "name": "Bob", "role": "GOVERNMENT" }
}
```

### Get Allocations by Status
```bash
curl -X GET "http://localhost:3000/api/allocations?status=PENDING&page=1&limit=5"
```

### Create an Allocation Request
```bash
curl -X POST http://localhost:3000/api/allocations \
  -H "Content-Type: application/json" \
  -d '{"fromOrgId":1,"toOrgId":2,"itemId":1,"quantity":100,"requestedBy":"alice@ngo.org"}'
```
**Response (201 Created):**
```json
{
  "message": "Allocation request created successfully",
  "data": { "id": 1, "status": "PENDING", "quantity": 100 }
}
```

### Update Allocation Status
```bash
curl -X PUT http://localhost:3000/api/allocations/1 \
  -H "Content-Type: application/json" \
  -d '{"status":"APPROVED","approvedBy":2}'
```

### Error Response (404 Not Found)
```bash
curl -X GET http://localhost:3000/api/users/999
```
**Response (404):**
```json
{ "error": "User not found" }
```

---

## Pagination, Filtering, and Error Semantics

### Pagination
All list endpoints support pagination via query parameters:
- `page` ‚Äî Page number (default: 1)
- `limit` ‚Äî Items per page (default: 10)

Example: `/api/users?page=2&limit=20`

### Filtering
- `/api/users?role=NGO` ‚Äî Filter users by role
- `/api/organizations?isActive=true` ‚Äî Filter active organizations
- `/api/allocations?status=PENDING&toOrgId=1` ‚Äî Filter by status and recipient

### HTTP Status Codes

| Code | Meaning               | Usage                              |
|------|-----------------------|------------------------------------|
| 200  | OK                    | Successful GET/PUT/DELETE          |
| 201  | Created               | Successful POST                    |
| 400  | Bad Request           | Invalid input or missing fields    |
| 404  | Not Found             | Resource does not exist            |
| 500  | Internal Server Error | Unexpected server-side error       |

---

## Reflection: Consistent Naming Improves Maintainability

### Benefits of RESTful Naming Conventions
1. **Predictability** ‚Äî Developers know `/api/users` returns users, not `/api/getUsers`
2. **Discoverability** ‚Äî Resource hierarchy is clear from URL structure
3. **Integration** ‚Äî Frontend teams can guess endpoints without documentation
4. **Tooling** ‚Äî REST clients (Postman, curl) work seamlessly

### Anti-patterns Avoided
- Verb-based routes (`/api/getUsers`, `/api/createOrder`)
- Inconsistent casing (`/api/Users` vs `/api/allocations`)
- Missing pagination on large collections
- Generic error messages without status codes



## 2.18 

---

## üåê Standardized API Response Format

### Why Standardized Responses Matter

Without a standard response format, every endpoint might return different shapes of data ‚Äî making it hard for frontend developers to handle results or errors predictably. Inconsistent responses increase code complexity and maintenance cost.

### The Unified Response Envelope

We define a common response format that **every endpoint** in our API follows:

#### Success Response Structure

```typescript
{
  "success": true,
  "message": string,
  "data": any,
  "timestamp": string,
  "pagination"?: {
    "page": number,
    "limit": number,
    "total": number,
    "totalPages": number
  }
}
```

#### Error Response Structure

```typescript
{
  "success": false,
  "message": string,
  "error": {
    "code": string,
    "details"?: any
  },
  "timestamp": string
}
```

### Benefits

‚úÖ **Consistency** ‚Äì Every API response follows the same structure  
‚úÖ **Predictable** ‚Äì Frontend can handle all responses uniformly  
‚úÖ **Debuggable** ‚Äì Error codes and timestamps aid troubleshooting  
‚úÖ **Observable** ‚Äì Easy integration with monitoring tools  
‚úÖ **Type-Safe** ‚Äì TypeScript interfaces ensure compile-time safety  

---

### Global Response Handler Implementation

We created a centralized response handler in [src/lib/responseHandler.ts](src/lib/responseHandler.ts):

```typescript
import { NextResponse } from "next/server";

export const sendSuccess = <T = unknown>(
  data: T,
  message = "Operation completed successfully",
  status = 200,
  pagination?: {
    page: number;
    limit: number;
    total: number;
    totalPages: number;
  }
) => {
  const response = {
    success: true,
    message,
    data,
    timestamp: new Date().toISOString(),
    ...(pagination && { pagination }),
  };

  return NextResponse.json(response, { status });
};

export const sendError = (
  message = "An unexpected error occurred",
  code = "INTERNAL_ERROR",
  status = 500,
  details?: unknown
) => {
  const response = {
    success: false,
    message,
    error: {
      code,
      ...(details && { details }),
    },
    timestamp: new Date().toISOString(),
  };

  console.error(`[API Error] ${code}: ${message}`, details);
  return NextResponse.json(response, { status });
};
```

---

### Standardized Error Codes

We maintain a dictionary of error codes in [src/lib/errorCodes.ts](src/lib/errorCodes.ts) for consistency:

```typescript
export const ERROR_CODES = {
  // Validation Errors (4xx)
  VALIDATION_ERROR: "VALIDATION_ERROR",
  MISSING_REQUIRED_FIELD: "MISSING_REQUIRED_FIELD",
  INVALID_INPUT: "INVALID_INPUT",
  INVALID_ID: "INVALID_ID",
  
  // Resource Errors (4xx)
  NOT_FOUND: "NOT_FOUND",
  ALLOCATION_NOT_FOUND: "ALLOCATION_NOT_FOUND",
  INVENTORY_NOT_FOUND: "INVENTORY_NOT_FOUND",
  ORGANIZATION_NOT_FOUND: "ORGANIZATION_NOT_FOUND",
  USER_NOT_FOUND: "USER_NOT_FOUND",
  
  // Business Logic Errors (4xx)
  INSUFFICIENT_STOCK: "INSUFFICIENT_STOCK",
  DUPLICATE_ENTRY: "DUPLICATE_ENTRY",
  INVALID_STATUS_TRANSITION: "INVALID_STATUS_TRANSITION",
  
  // Server Errors (5xx)
  INTERNAL_ERROR: "INTERNAL_ERROR",
  DATABASE_ERROR: "DATABASE_ERROR",
  SERVER_ERROR: "SERVER_ERROR",
} as const;
```

Using these codes makes it easier to trace issues from logs or monitoring dashboards.

---

### Usage Examples in API Routes

#### Example 1: GET /api/allocations

```typescript
import { sendSuccess, sendError } from "@/lib/responseHandler";
import { ERROR_CODES } from "@/lib/errorCodes";

export async function GET(req: Request) {
  try {
    const allocations = await prisma.allocation.findMany({
      include: {
        fromOrg: { select: { id: true, name: true } },
        toOrg: { select: { id: true, name: true } },
      },
      skip,
      take: limit,
    });

    return sendSuccess(
      allocations,
      "Allocations retrieved successfully",
      200,
      { page, limit, total, totalPages }
    );
  } catch (error) {
    return sendError(
      "Failed to fetch allocations",
      ERROR_CODES.DATABASE_ERROR,
      500,
      error
    );
  }
}
```

#### Example 2: POST /api/allocations

```typescript
export async function POST(req: Request) {
  try {
    const { toOrgId, itemId, quantity } = await req.json();

    if (!toOrgId || !itemId || !quantity) {
      return sendError(
        "Missing required fields: toOrgId, itemId, quantity",
        ERROR_CODES.MISSING_REQUIRED_FIELD,
        400
      );
    }

    if (quantity <= 0) {
      return sendError(
        "Quantity must be greater than 0",
        ERROR_CODES.INVALID_INPUT,
        400
      );
    }

    const allocation = await prisma.allocation.create({
      data: { toOrgId, itemId, quantity, status: "PENDING" },
    });

    return sendSuccess(
      allocation,
      "Allocation request created successfully",
      201
    );
  } catch (error) {
    return sendError(
      "Failed to create allocation",
      ERROR_CODES.DATABASE_ERROR,
      500,
      error
    );
  }
}
```

#### Example 3: GET /api/allocations/:id

```typescript
export async function GET(req: Request, { params }: Params) {
  try {
    const { id } = await params;
    const allocationId = parseInt(id, 10);

    if (isNaN(allocationId)) {
      return sendError(
        "Invalid allocation ID",
        ERROR_CODES.INVALID_ID,
        400
      );
    }

    const allocation = await prisma.allocation.findUnique({
      where: { id: allocationId },
    });

    if (!allocation) {
      return sendError(
        "Allocation not found",
        ERROR_CODES.ALLOCATION_NOT_FOUND,
        404
      );
    }

    return sendSuccess(allocation, "Allocation retrieved successfully");
  } catch (error) {
    return sendError(
      "Failed to fetch allocation",
      ERROR_CODES.DATABASE_ERROR,
      500,
      error
    );
  }
}
```

---

### Example API Responses

#### ‚úÖ Success Response (List with Pagination)

```json
{
  "success": true,
  "message": "Allocations retrieved successfully",
  "data": [
    {
      "id": 1,
      "fromOrg": { "id": 2, "name": "Red Cross India" },
      "toOrg": { "id": 3, "name": "District Relief Center" },
      "quantity": 100,
      "status": "APPROVED"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 10,
    "total": 45,
    "totalPages": 5
  },
  "timestamp": "2026-01-30T10:00:00.000Z"
}
```

#### ‚úÖ Success Response (Single Resource)

```json
{
  "success": true,
  "message": "Allocation created successfully",
  "data": {
    "id": 12,
    "toOrgId": 3,
    "itemId": 5,
    "quantity": 50,
    "status": "PENDING",
    "requestDate": "2026-01-30T10:00:00.000Z"
  },
  "timestamp": "2026-01-30T10:00:00.000Z"
}
```

#### ‚ùå Error Response (Validation)

```json
{
  "success": false,
  "message": "Missing required fields: toOrgId, itemId, quantity",
  "error": {
    "code": "MISSING_REQUIRED_FIELD"
  },
  "timestamp": "2026-01-30T10:00:00.000Z"
}
```

#### ‚ùå Error Response (Not Found)

```json
{
  "success": false,
  "message": "Allocation not found",
  "error": {
    "code": "ALLOCATION_NOT_FOUND"
  },
  "timestamp": "2026-01-30T10:00:00.000Z"
}
```

#### ‚ùå Error Response (Server Error)

```json
{
  "success": false,
  "message": "Failed to fetch allocations",
  "error": {
    "code": "DATABASE_ERROR",
    "details": {
      "name": "PrismaClientKnownRequestError",
      "message": "Connection timeout"
    }
  },
  "timestamp": "2026-01-30T10:00:00.000Z"
}
```

---

### API Endpoints Coverage

All API routes now use standardized responses:

| Endpoint | Methods | Handler Used |
|----------|---------|--------------|
| `/api/allocations` | GET, POST | ‚úÖ Standardized |
| `/api/allocations/:id` | GET, PUT, DELETE | ‚úÖ Standardized |
| `/api/inventory` | GET, POST | ‚úÖ Standardized |
| `/api/inventory/:id` | GET, PUT, DELETE | ‚úÖ Standardized |
| `/api/organizations` | GET, POST | ‚úÖ Standardized |
| `/api/organizations/:id` | GET, PUT, DELETE | ‚úÖ Standardized |
| `/api/users` | GET, POST | ‚úÖ Standardized |
| `/api/users/:id` | GET, PUT, DELETE | ‚úÖ Standardized |

---

### Developer Experience Benefits

1. **Faster Debugging** ‚Äì Every error has a code and timestamp for quick identification
2. **Reliable Frontend** ‚Äì All responses share the same schema, reducing conditional logic
3. **Easy Monitoring** ‚Äì Integrate with Sentry, Datadog, or custom dashboards effortlessly
4. **Team Onboarding** ‚Äì New developers instantly understand the response format
5. **Type Safety** ‚Äì TypeScript interfaces prevent response structure mistakes

---

### Observability & Logging

The `sendError` handler automatically logs errors with context:

```typescript
console.error(`[API Error] ${code}: ${message}`, details);
```

This enables:
- Centralized error tracking
- Performance monitoring
- Alert configuration for critical errors
- Debug traces for production issues

---

### Summary

**Standardized API responses** ensure consistency, predictability, and maintainability across the entire application. By using a unified response handler and error codes:

‚úÖ Frontend developers work more efficiently  
‚úÖ Bugs are easier to trace and fix  
‚úÖ Monitoring tools integrate seamlessly  
‚úÖ Code reviews focus on logic, not response formats  
‚úÖ New team members onboard faster  

**Think of the global response handler as your project's "API voice" ‚Äî every endpoint speaks in the same tone, no matter who wrote it.**


---

## 2.23 Lesson: Redis Caching for Performance Optimization

### Why Caching Matters

Every database query consumes server resources and adds latency. Caching is a **critical performance optimization strategy** that stores frequently accessed data in memory, enabling near-instant responses for repeated requests.

#### Performance Impact Example

| Scenario | Response Time | DB Load |
|----------|---|---|
| **Without Cache** | ~150ms per request | All requests hit database |
| **With Redis Cache (Hit)** | ~5-10ms | Zero database queries |
| **Performance Gain** | **15-30x faster** | **Reduced by ~80%** |

---

### 1. Installation & Setup

#### Install ioredis Package

```bash
npm install ioredis
npm install --save-dev @types/ioredis
```

#### Create Redis Connection Utility

**File: `lib/redis.ts`**

```typescript
import Redis from "ioredis";

/**
 * Redis client initialization
 * Connects to Redis using environment variable or localhost fallback
 *
 * Environment Variables:
 * - REDIS_URL: Full Redis connection string (e.g., redis://localhost:6379)
 *
 * Tip: Always use environment variables for credentials - never hardcode!
 */
const redis = new Redis(process.env.REDIS_URL || "redis://localhost:6379");

// Handle connection events
redis.on("connect", () => {
  console.log("‚úÖ Redis connected");
});

redis.on("error", (error) => {
  console.error("‚ùå Redis connection error:", error.message);
});

export default redis;
```

---

### 2. Cache-Aside Pattern (Lazy Loading)

This is the most common and practical caching pattern used in our application:

```
Client Request
    ‚Üì
Check Redis Cache
    ‚îú‚îÄ HIT ‚Üí Return cached data (fast!)
    ‚îî‚îÄ MISS ‚Üí 
        ‚îú‚îÄ Query database
        ‚îú‚îÄ Store in cache (with TTL)
        ‚îî‚îÄ Return response
```

#### Example: Caching Users List

**File: `app/api/users/route.ts`**

```typescript
export async function GET(req: NextRequest) {
  try {
    const { searchParams } = new URL(req.url);
    const page = Number(searchParams.get("page")) || 1;
    const limit = Number(searchParams.get("limit")) || 10;
    const role = searchParams.get("role");
    const skip = (page - 1) * limit;

    // Create cache key based on query parameters
    const cacheKey = `users:list:${page}:${limit}:${role || "all"}`;
    
    // Check Redis cache first (Cache-Aside Pattern)
    const cachedData = await redis.get(cacheKey);
    if (cachedData) {
      console.log(`‚úÖ Cache Hit: ${cacheKey}`);
      return sendSuccess(
        JSON.parse(cachedData),
        "Users retrieved successfully (from cache)",
        200
      );
    }

    console.log(`‚ö†Ô∏è Cache Miss: ${cacheKey} - Fetching from database`);

    const where = role ? { role: role as "NGO" | "GOVERNMENT" } : undefined;

    const [users, total] = await Promise.all([
      prisma.user.findMany({
        ...(where && { where }),
        select: {
          id: true,
          email: true,
          name: true,
          role: true,
          organizationId: true,
          organization: { select: { id: true, name: true } },
          createdAt: true,
        },
        skip,
        take: limit,
        orderBy: { createdAt: "desc" },
      }),
      prisma.user.count(where ? { where } : undefined),
    ]);

    const responseData = users;
    const pagination = {
      page,
      limit,
      total,
      totalPages: Math.ceil(total / limit),
    };

    // Cache the response for 5 minutes (300 seconds)
    await redis.setex(
      cacheKey,
      300,
      JSON.stringify({ data: responseData, pagination })
    );

    return sendSuccess(responseData, "Users retrieved successfully", 200, pagination);
  } catch (error) {
    return handleDatabaseError(error, "GET /api/users");
  }
}
```

---

### 3. TTL Policy (Time-To-Live)

TTL determines how long data remains in cache before expiring:

| Data Type | TTL | Rationale |
|-----------|-----|-----------|
| **User Lists** | 5 min (300s) | Users don't change frequently |
| **Organization Details** | 5 min (300s) | Relatively stable data |
| **Single User** | 10 min (600s) | Individual records are more stable |
| **Allocations** | 3 min (180s) | Changes more frequently |
| **Individual Allocation** | 5 min (300s) | Intermediate frequency |

**Why Different TTLs?**
- **Longer TTLs** = Higher performance, higher stale data risk
- **Shorter TTLs** = Lower stale data risk, more database hits
- **Data criticality** determines the trade-off

---

### 4. Cache Invalidation Strategy

When data changes, we must invalidate stale cache entries. We implemented **aggressive invalidation** to maintain data coherence:

#### Single Resource Update

**File: `app/api/users/[id]/route.ts`**

```typescript
export async function PUT(req: Request, { params }: Params) {
  try {
    const { id } = await params;
    const userId = parseInt(id, 10);

    // ... validation and update logic ...

    const updatedUser = await prisma.user.update({
      where: { id: userId },
      data: validatedData,
      select: {
        id: true,
        email: true,
        name: true,
        role: true,
        organizationId: true,
        updatedAt: true,
      },
    });

    // Invalidate caches after update
    await redis.del(`user:${userId}`); // Invalidate specific user cache
    const keys = await redis.keys("users:list:*"); // Find all list caches
    if (keys.length > 0) {
      await redis.del(...keys); // Delete all at once
    }
    console.log(`üóëÔ∏è Cache Invalidated: user:${userId} and users:list:* patterns`);

    return sendSuccess(updatedUser, "User updated successfully");
  } catch (error) {
    // ... error handling ...
  }
}
```

#### Resource Creation

**File: `app/api/users/route.ts` (POST)**

```typescript
export async function POST(req: NextRequest) {
  try {
    // ... validation and creation logic ...

    const user = await prisma.user.create({
      data: {
        email: validatedData.email,
        name: validatedData.name,
        passwordHash: validatedData.passwordHash,
        role: validatedData.role,
        organizationId: validatedData.organizationId || null,
      },
      select: {
        id: true,
        email: true,
        name: true,
        role: true,
        organizationId: true,
        createdAt: true,
      },
    });

    // Invalidate all user list caches when new user is created
    const keys = await redis.keys("users:list:*");
    if (keys.length > 0) {
      await redis.del(...keys);
      console.log(`üóëÔ∏è Cache Invalidated: Cleared ${keys.length} user list caches`);
    }

    return createSuccessResponse("User created successfully", user, 201);
  } catch (error) {
    // ... error handling ...
  }
}
```

---

### 5. Caching Strategy Summary

#### Endpoints Cached in ReliefSync

| Endpoint | TTL | Cache Key Pattern | Invalidation |
|----------|-----|---|---|
| `GET /api/users` | 5 min | `users:list:{page}:{limit}:{role}` | On POST/PUT/DELETE user |
| `GET /api/users/:id` | 10 min | `user:{id}` | On PUT/DELETE that user |
| `GET /api/organizations` | 5 min | `organizations:list:{page}:{limit}:{isActive}` | On POST/PUT/DELETE org |
| `GET /api/organizations/:id` | 10 min | `organization:{id}` | On PUT/DELETE that org |
| `GET /api/allocations` | 3 min | `allocations:list:{page}:{limit}:{filters}` | On POST/PUT/DELETE allocation |
| `GET /api/allocations/:id` | 5 min | `allocation:{id}` | On PUT/DELETE that allocation |

---

### 6. Cache Coherence & Stale Data Management

#### Potential Stale Data Scenarios

**Scenario 1: User Updates Between Cache Writes**
```
Time 0: User A fetches user list ‚Üí cached
Time 30s: User B updates user name ‚Üí cache not immediately invalidated
Time 40s: User A fetches again ‚Üí gets stale data (name unchanged)
Risk: HIGH if TTL too long
```

**Mitigation:**
- Keep TTLs reasonable (3-10 minutes)
- Aggressive cache invalidation on writes
- Consider cache versioning for critical data

#### Our Approach

1. **Aggressive Invalidation** ‚Äì Clear related caches on any write
2. **Short TTLs** ‚Äì Lower risk window for stale data
3. **Pattern-Based Deletion** ‚Äì Invalidate all variations of a resource list
4. **Logging** ‚Äì Track when caches are invalidated for debugging

---

### 7. When NOT to Cache

‚ö†Ô∏è **Don't cache:**
- Real-time critical data (active disaster zones, emergency allocations)
- Highly personalized data (user-specific permissions)
- Frequently changing metrics
- Large datasets that rarely get reused

‚úÖ **Good candidates for caching:**
- Organization master data
- User role & permission info (with short TTL)
- List views with pagination
- Historical/analytical data

---

### 8. Testing Cache Behavior

#### Step 1: Cold Start (Cache Miss)

```bash
curl -X GET http://localhost:3000/api/users
```

**Terminal Output:**
```
‚ö†Ô∏è Cache Miss: users:list:1:10:all - Fetching from database
```

**Response Time:** ~120-150ms (database query)

#### Step 2: Warm Cache (Cache Hit)

```bash
curl -X GET http://localhost:3000/api/users
```

**Terminal Output:**
```
‚úÖ Cache Hit: users:list:1:10:all
```

**Response Time:** ~5-10ms (Redis retrieval)

#### Step 3: Verify Invalidation

```bash
curl -X POST http://localhost:3000/api/users \
  -H "Content-Type: application/json" \
  -d '{"email":"new@test.com","name":"New User","passwordHash":"hash","role":"NGO"}'
```

**Terminal Output:**
```
üóëÔ∏è Cache Invalidated: Cleared 1 user list caches
```

```bash
curl -X GET http://localhost:3000/api/users
```

**Terminal Output:**
```
‚ö†Ô∏è Cache Miss: users:list:1:10:all - Fetching from database
```

---

### 9. Reflection: Caching Trade-offs

| Aspect | Benefit | Risk |
|--------|---------|------|
| **Performance** | 10-30x faster repeated requests | Complexity in invalidation logic |
| **DB Load** | Reduced by 80%+ | Must monitor cache hit rates |
| **Consistency** | Fresh data with invalidation strategy | Stale data window if TTL too long |
| **Scalability** | Enables higher traffic capacity | Redis server becomes critical component |
| **Latency** | Milliseconds vs hundreds of ms | Network hop to Redis (minimal) |

---

### 10. Pro Tips for Production

1. **Monitor Cache Hit Rates**
   ```typescript
   // Track hits vs misses for metrics
   let cacheHits = 0, cacheMisses = 0;
   ```

2. **Use Cache Warming** ‚Äì Pre-load frequently accessed data on startup

3. **Implement Cache Versioning** ‚Äì Add version to key if schema changes
   ```typescript
   const cacheKey = `users:list:v2:${page}:${limit}:${role}`;
   ```

4. **Set Up Redis Persistence** ‚Äì Don't lose cache on restart
   ```
   # redis.conf
   save 900 1      # Save after 900 seconds if 1 key changed
   appendonly yes  # Enable AOF persistence
   ```

5. **Use Redis CLI for Debugging**
   ```bash
   redis-cli
   > KEYS users:* # See all user-related keys
   > GET user:1   # Check specific key value
   > FLUSHDB      # Clear all cache (use carefully!)
   ```

---

### Summary

**Redis caching transforms your application from database-bound to memory-bound**, enabling:
- ‚úÖ 10-30x performance improvement for repeated requests
- ‚úÖ 80%+ reduction in database load
- ‚úÖ Ability to scale with user growth
- ‚úÖ Better user experience with sub-10ms response times

**The key to successful caching:** Balance speed with data freshness through smart invalidation and reasonable TTLs.

**Remember:** *"Cache is like a short-term memory ‚Äî it makes things fast, but only if you remember to forget at the right time."*

````